"""
Hugging Face Hub ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê¸°ë°˜ ë²¡í„°í™” ëª¨ë“ˆ
- HF Hubì—ì„œ ëª¨ë¸ì„ ìë™ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ ì¶”ë¡ 
- API ì—”ë“œí¬ì¸íŠ¸ ì˜ì¡´ ì—†ìŒ (410/404 ì—ëŸ¬ ì›ì²œ ì°¨ë‹¨)
- ë¡œì»¬/Streamlit Cloud ëª¨ë‘ ë™ì¼í•˜ê²Œ ë™ì‘
"""

import numpy as np
from transformers import AutoTokenizer, AutoModel
import torch
from typing import List, Optional
import os


class HuggingFaceAPIVectorizer:
    """
    Hugging Face Hubì—ì„œ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¡œì»¬ ì¶”ë¡ í•˜ëŠ” ë²¡í„°í™” í´ë˜ìŠ¤

    - API í˜¸ì¶œ ëŒ€ì‹  ëª¨ë¸ íŒŒì¼ì„ ì§ì ‘ ë‹¤ìš´ë¡œë“œ í›„ ì¶”ë¡ 
    - torch + transformers ì‚¬ìš© (BERTVectorizerì™€ ë™ì¼í•œ ì¶”ë¡  ë°©ì‹)
    - ìµœì´ˆ 1íšŒë§Œ ë‹¤ìš´ë¡œë“œ, ì´í›„ HF ìºì‹œì—ì„œ ë¡œë“œ
    """

    def __init__(
        self,
        model_id: str = "fullfish/multicampus_semantic",
        api_token: Optional[str] = None,
    ):
        """
        Args:
            model_id: Hugging Face ëª¨ë¸ ID (ì˜ˆ: "fullfish/multicampus_semantic")
            api_token: Hugging Face API í† í° (private ëª¨ë¸ì¼ ê²½ìš° í•„ìš”)
        """
        self.model_id = model_id
        self.api_token = api_token or os.getenv("HF_TOKEN")

        print(f"ğŸ”„ Hugging Face Hubì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_id}")

        # HF Hubì—ì„œ ëª¨ë¸ + í† í¬ë‚˜ì´ì € ë‹¤ìš´ë¡œë“œ (ìºì‹œë¨)
        token = self.api_token if self.api_token else None
        self.tokenizer = AutoTokenizer.from_pretrained(model_id, token=token)
        self.model = AutoModel.from_pretrained(model_id, token=token)
        self.model.eval()

        # CPU ì‚¬ìš© (Streamlit CloudëŠ” GPU ì—†ìŒ)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model.to(self.device)

        if self.device.type == "cuda":
            self.model.half()

        print(f"âœ“ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")
        print(f"  - Model: {model_id}")
        print(f"  - Device: {self.device}")
        print(f"  - Hidden Size: {self.model.config.hidden_size}")

    def encode(self, text: str, max_length: int = 512) -> np.ndarray:
        """
        ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜ (Mean Pooling)

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            max_length: ìµœëŒ€ í† í° ê¸¸ì´

        Returns:
            768ì°¨ì› ë²¡í„° (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
        """
        if not text or not text.strip():
            return np.zeros(self.model.config.hidden_size)

        # í† í°í™”
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=max_length,
            truncation=True,
            padding=True,
        )

        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        inputs = {k: v.to(self.device) for k, v in inputs.items()}

        # ì¶”ë¡ 
        with torch.no_grad():
            outputs = self.model(**inputs, return_dict=True)

        # Mean Pooling (attention_mask ê³ ë ¤)
        attention_mask = inputs["attention_mask"]
        token_embeddings = outputs.last_hidden_state

        input_mask_expanded = (
            attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        )
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
        sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)

        mean_embedding = (sum_embeddings / sum_mask).cpu().numpy()[0]
        return mean_embedding

    def encode_batch(
        self,
        texts: List[str],
        batch_size: int = 16,
        max_length: int = 512,
        show_progress: bool = False,
    ) -> np.ndarray:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„°í™”

        Args:
            texts: ì…ë ¥ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            batch_size: ë°°ì¹˜ í¬ê¸°
            max_length: ìµœëŒ€ í† í° ê¸¸ì´
            show_progress: ì§„í–‰ìƒí™© í‘œì‹œ ì—¬ë¶€

        Returns:
            (len(texts), hidden_size) í¬ê¸°ì˜ numpy ë°°ì—´
        """
        all_embeddings = []

        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]

            if show_progress:
                print(f"ì²˜ë¦¬ ì¤‘: {i}/{len(texts)}")

            # ë¹ˆ í…ìŠ¤íŠ¸ ì²˜ë¦¬
            processed = [t if t and t.strip() else " " for t in batch]

            inputs = self.tokenizer(
                processed,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding=True,
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                outputs = self.model(**inputs, return_dict=True)

            attention_mask = inputs["attention_mask"]
            token_embeddings = outputs.last_hidden_state

            input_mask_expanded = (
                attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
            )
            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, dim=1)
            sum_mask = torch.clamp(input_mask_expanded.sum(dim=1), min=1e-9)

            batch_embeddings = (sum_embeddings / sum_mask).cpu().numpy()
            all_embeddings.append(batch_embeddings)

        return np.vstack(all_embeddings)

    def get_vector_size(self) -> int:
        """ë²¡í„° ì°¨ì› ë°˜í™˜"""
        return self.model.config.hidden_size


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ìºì‹±
_hf_vectorizer_instance = None


def get_hf_api_vectorizer(
    model_id: str = "fullfish/multicampus_semantic",
    api_token: Optional[str] = None,
) -> HuggingFaceAPIVectorizer:
    """
    HuggingFaceAPIVectorizer ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

    Args:
        model_id: Hugging Face ëª¨ë¸ ID
        api_token: Hugging Face API í† í° (private ëª¨ë¸ì¼ ê²½ìš° í•„ìš”)

    Returns:
        HuggingFaceAPIVectorizer ì¸ìŠ¤í„´ìŠ¤
    """
    global _hf_vectorizer_instance

    if _hf_vectorizer_instance is None:
        _hf_vectorizer_instance = HuggingFaceAPIVectorizer(
            model_id=model_id, api_token=api_token
        )

    return _hf_vectorizer_instance
